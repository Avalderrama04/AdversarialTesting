## About
Adversarial attacks attempt to deceive learning models by implementing a defective input,
compromising the effectiveness as a result. We explore the mechanisms behind attacks on convolutional
neural networks (CNNs). This project aims to contribute to the development of safer ML models to ensure
their reliability, through evaluating the effectiveness of adversarial attacks and anomaly detection models on image perturbation algorithms.




## Highlights
- utilize dataset of brain tumor mri images, can train and import your own data
- SimBA & C&W attacks prove effectiveness in fooling the MRI imaging model
- Anomaly detection utilized to spot image perturbations
- Gemini API provides chatbot and diagnostic summaries


## Example
1. Run `train.py` on a dataset of brain tumor MRI images
2. Run `attack.py` to initialize the C&W attack on your ML model and fool it
3. Run `simba.py` to initialize the SimBA attack on your model and fool it
4. Run `anomaly.py` to train an anomaly detection model from the adversarial perturbations generated by the attacks
5. Run `app.py` to launch an interface to upload your images between the normal vs. adversarial models, and have a Gemini API provide a chatbot and diagnostic summaries. 

## Acknowledgements
References
[1] N. Carlini and D. Wagner, Towards evaluating the robustness of neural networks (2017), arXiv:1608.04644
[cs.CR] .
[2] C. Guo, J. R. Gardner, Y. You, A. G. Wilson, and K. Q. Weinberger, Simple black-box adversarial attacks (2019),
arXiv:1905.07121 [cs.LG] .
[3] T. R. Sarkar, N. Das, P. S. Maitra, B. Some, R. Saha, O. Adhikary, B. Bose, and J. Sen, Evaluating adversarial
robustness: A comparison of fgsm, carlini-wagner attacks, and the role of distillation as defense mechanism
(2024), arXiv:2404.04245 [cs.CR] .
[4] M. Nickparvar, Brain tumor mri dataset (2021).
[5] Carco-git, Cw_attack_on_mnist, https://github.com/Carco-git/CW_Attack_on_MNIST (2023),
accessed: 2025-04-21.
[6] cg563, Simple_blackbox-attack, https://github.com/cg563/simple-blackbox-attack/tree/
master (2023), accessed: 2025-04-21
